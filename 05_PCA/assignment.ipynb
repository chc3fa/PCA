{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA and Text Analysis\n",
    "\n",
    "This assignment involves processing real e-mails, some of which are scams. Some of these scam e-mails have some offensive content. I don't think anything is worse than R-rated, but I just want to warn you that if you start reading the e-mail text, you might read something offensive. If that's a problem, feel free to e-mail me and we can talk about it more or you can skip the assignment.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. \n",
    "\n",
    "Open the `Phishing_Email.parquet` data. It is available at `https://data434.s3.us-east-2.amazonaws.com/Phishing_Email.parquet`, and you can download it directly using Pandas by providing that URL: `df = pd.read_parquet('https://data434.s3.us-east-2.amazonaws.com/Phishing_Email.parquet')`.\n",
    "\n",
    "We just want to look at the first step of cleaning text data, so you can get an idea of how it works. The `Email Text` variable contains the actual text of the email and the `Email Type` takes the value `Phishing Email` or `Safe Email`. We want to predict which emails are phishing emails from their contents. \n",
    "\n",
    "Use the `str.split()` method to break the `Phishing Email` values into **tokens**: The individual words or symbols that create text data like emails. Natural Language Processing is primarily about analyzing the frequency and co-occurrence of tokens. Print the results of your split and examine it.\n",
    "\n",
    "In words, how would you clean the tokens and use them to predict whether the email is a phishing scam or not? A short summary of the kinds of tasks you would do and how you would run a predictive algorithm is fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Email Text</th>\n",
       "      <th>Email Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>re : 6 . 1100 , disc : uniformitarianism , re ...</td>\n",
       "      <td>Safe Email</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>the other side of * galicismos * * galicismo *...</td>\n",
       "      <td>Safe Email</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>re : equistar deal tickets are you still avail...</td>\n",
       "      <td>Safe Email</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>\\nHello I am your hot lil horny toy.\\n    I am...</td>\n",
       "      <td>Phishing Email</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>software at incredibly low prices ( 86 % lower...</td>\n",
       "      <td>Phishing Email</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  Unnamed: 0                                         Email Text  \\\n",
       "0      0           0  re : 6 . 1100 , disc : uniformitarianism , re ...   \n",
       "1      1           1  the other side of * galicismos * * galicismo *...   \n",
       "2      2           2  re : equistar deal tickets are you still avail...   \n",
       "3      3           3  \\nHello I am your hot lil horny toy.\\n    I am...   \n",
       "4      4           4  software at incredibly low prices ( 86 % lower...   \n",
       "\n",
       "       Email Type  \n",
       "0      Safe Email  \n",
       "1      Safe Email  \n",
       "2      Safe Email  \n",
       "3  Phishing Email  \n",
       "4  Phishing Email  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1\n",
    "import pandas as pd\n",
    "df = pd.read_parquet('https://data434.s3.us-east-2.amazonaws.com/Phishing_Email.parquet')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['re', ':', '6', '.', '1100', ',', 'disc', ':', 'uniformitarianism', ',', 're', ':', '1086', ';', 'sex', '/', 'lang', 'dick', 'hudson', \"'s\"]\n",
      "['the', 'other', 'side', 'of', '*', 'galicismos', '*', '*', 'galicismo', '*', 'is', 'a', 'spanish', 'term', 'which', 'names', 'the', 'improper', 'introduction', 'of']\n",
      "['re', ':', 'equistar', 'deal', 'tickets', 'are', 'you', 'still', 'available', 'to', 'assist', 'robert', 'with', 'entering', 'the', 'new', 'deal', 'tickets', 'for', 'equistar']\n",
      "['Hello', 'I', 'am', 'your', 'hot', 'lil', 'horny', 'toy.', 'I', 'am', 'the', 'one', 'you', 'dream', 'About,', 'I', 'am', 'a', 'very', 'open']\n",
      "['software', 'at', 'incredibly', 'low', 'prices', '(', '86', '%', 'lower', ')', '.', 'drapery', 'seventeen', 'term', 'represent', 'any', 'sing', '.', 'feet', 'wild']\n",
      "['global', 'risk', 'management', 'operations', 'sally', 'congratulations', 'on', 'your', 'new', 'role', '.', 'if', 'you', 'were', 'not', 'already', 'aware', ',', 'i', 'am']\n",
      "['On', 'Sun,', 'Aug', '11,', '2002', 'at', '11:17:47AM', '+0100,', 'wintermute', 'mentioned:', '>', '>', 'The', 'impression', 'I', 'get', 'from', 'reading', 'lkml', 'the']\n",
      "['entourage', ',', 'stockmogul', 'newsletter', 'ralph', 'velez', ',', 'genex', 'pharmaceutical', ',', 'inc', '.', '(', 'otcbb', ':', 'genx', ')', 'biotech', 'sizzle', 'with']\n",
      "['we', 'owe', 'you', 'lots', 'of', 'money', 'dear', 'applicant', ',', 'after', 'further', 'review', 'upon', 'receiving', 'your', 'application', 'your', 'current', 'mortgage', 'qualifies']\n",
      "['re', ':', 'coastal', 'deal', '-', 'with', 'exxon', 'participation', 'under', 'the', 'project', 'agreement', 'thanks', 'for', 'the', 'info', '!', 'as', 'greg', 'mentioned']\n"
     ]
    }
   ],
   "source": [
    "#1 cont.\n",
    "token_list = df['Email Text'].str.split()\n",
    "for tokens in token_list[:10]:\n",
    "    print(tokens[:20])\n",
    "    # for token in tokens:\n",
    "        # print(token)\n",
    "\n",
    "#I would clean the tokens by removing characters like '*' or ':' that don't have much meaning. I would then use the tokens to see if certain words that evoke feelings of\n",
    "# needed action or urgency to do something so emails that had many tokens like \"now\" or \"immediately\" or \"alert\" would be useful to see if emails are phishing scams.\n",
    "# I could run PCA on the tokens of the emails by first doing one-hot encoding on the tokens to convert them to numerical variables. \n",
    "# I would then standardize he range of the tokens so that each one of them contributes equally to the analysis. I can do this taking the \n",
    "# mean and dividing by the standard deviation for each value of each variable. Then I can use PCA to see if the presence of each variable \n",
    "# helps tell if the email is a scam or not.\n",
    "\n",
    "# source: article on PCA, https://builtin.com/data-science/step-step-explanation-principal-component-analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2. \n",
    "\n",
    "I aggregated all the emails into a single vector, and removed the punctuation and very common words (e.g. \"the\"). Run the below code chunk to open it, and use the Counter class to look at the most common words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#2\n",
    "import pickle\n",
    "\n",
    "with open('all_tokens.pickle', 'rb') as file:\n",
    "    all_tokens = pickle.load(file)\n",
    "    \n",
    "from collections import Counter\n",
    "token_count = Counter(all_tokens)\n",
    "token_freq = token_count.most_common()\n",
    "\n",
    "# print(token_count)\n",
    "# print(token_freq)\n",
    "\n",
    "df2 = pd.DataFrame(token_freq, columns=['token', 'count'])\n",
    "df2.head()\n",
    "\n",
    "df2['count'].hist(bins=200)\n",
    "\n",
    "#I notice that most tokens appear near 0 times, and only very few number of tokens appear more than a few times.  The histogram looks like large \n",
    "# grouping of tokens that appear once or barely more than once, with tokens that appear more than a few times being very rare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a histogram of the occurrences of tokens. What do you notice about the frequency of occurrence of different tokens? How does it look?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3. \n",
    "\n",
    "Load `Phishing_clean.parquet`. This is the text from the e-mails broken into the most common 2,711 tokens and one-hot-encoded as features/covariates. So each row is an e-mail, the `Email Type` takes the value 1 if it's a scam and 0 otherwise, and every other column is a word or symbol that occurs in at least 15 e-mails.\n",
    "\n",
    "1. Perform an 80/20 train-test split of the data.\n",
    "2. Run a regression of $y$ on the one-hot-encoded emails. What is the $R^2$ on the test set? On the training set?\n",
    "3. What words have the largest coefficients in absolute value and most strongly influence predictions? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/aidanodonnell/PCA-1/05_PCA/assignment.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/aidanodonnell/PCA-1/05_PCA/assignment.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m train_test_split\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/aidanodonnell/PCA-1/05_PCA/assignment.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlinear_model\u001b[39;00m \u001b[39mimport\u001b[39;00m LinearRegression\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/aidanodonnell/PCA-1/05_PCA/assignment.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m df3 \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_parquet(\u001b[39m'\u001b[39m\u001b[39mPhishing_clean.parquet\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/aidanodonnell/PCA-1/05_PCA/assignment.ipynb#X11sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m X \u001b[39m=\u001b[39m df3\u001b[39m.\u001b[39mdrop(\u001b[39m'\u001b[39m\u001b[39mEmail Type\u001b[39m\u001b[39m'\u001b[39m, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/aidanodonnell/PCA-1/05_PCA/assignment.ipynb#X11sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m Y \u001b[39m=\u001b[39m df3[\u001b[39m'\u001b[39m\u001b[39mEmail Type\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "#3\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "df3 = pd.read_parquet('Phishing_clean.parquet')\n",
    "\n",
    "# X = df3.drop('Email Type', axis=1)\n",
    "# Y = df3['Email Type']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, \n",
    "                                                    test_size=0.2, random_state=100)\n",
    "\n",
    "reg = LinearRegression().fit(X_train, Y_train)\n",
    "print('R-squared: ', reg.score(X_train, Y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4. \n",
    "\n",
    "Take the matrix of one-hot-encoded tokens (the data, less the outcome variable, `Email Type`) and perform a principal components analysis decomposition with two components. Plot the first two principal components in a scatter plot, and hue the points by whether they are a phishing scam or not. Do you notice any patterns?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5.\n",
    "\n",
    "Run a linear regression of $y$ on the first 2,610 principal components of $X$. What is the $R^2$ on the training and test sets? (I used cross validation to determine that 2,610 was approximately optimal, but not all 2,711 components.)\n",
    "\n",
    "How does this performance compare to the linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6.\n",
    "\n",
    "Explain briefly in your own words what the advantage is in using the principal components to run this high-dimensional regression, rather than the original data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "317be54081e4a1796c7d85ce90db902ddb84434b38a99bb6cb53d1ca93569633"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
